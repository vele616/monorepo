---
title: "Big Data Lead"
location: "Remote"
host: "https://careers.smartrecruiters.com/DemandMatrix/?remoteLocation=true"
companyName: "DemandMatrix"
url: "https://jobs.smartrecruiters.com/DemandMatrix/743999730247313-big-data-lead"
applyUrl: "https://jobs.smartrecruiters.com/oneclick-ui/company/DemandMatrix/publication/964c563c-1eb0-490c-9a4c-8b6aedb16ae9?dcr_id=DCRA1"
timestamp: 1613520000000
hashtags: "#python,#spark,#hadoop,#windows,#googlecloud,#ios,#kubernetes,#docker,#aws,#ui/ux"
jobType: "software"
logoUrl: "https://jobboard-logos-bucket.s3.eu-central-1.amazonaws.com/demandmatrix"
companyWebsite: "http://www.demandmatrix.com/"
summary: "DemandMatrix is searching for a Big Data Lead that has 3 years of experience to build services and pipelines using Python."
summaryBackup: "DemandMatrix is looking for a big data lead that has experience in: #python, #googlecloud, #spark."
featured: 8
archived: "true"
---

## Company Description

At DemandMatrix, our vision is to disrupt the $100 billion sales and marketing intelligence industry by using domain knowledge, machine learning and AI. Fortune 100 companies like Microsoft, Google, Adobe, Amazon, IBM trust us to identify their next customer.

## Job Description

## What will you do?

*   To help us go to the next level we are looking to onboard a hands-on SME in leveraging big data tech to solve the most complex data issues. You will spend almost half of time with hands-on coding.
*   It involves large scale text data processing, event driven data pipelines, in-memory computations, optimization considering CPU core to network IO to disk IO.
*   You will be using cloud native services in AWS and GCP.

## Who Are You? 

*   Solid grounding in computer engineering, Unix, data structures and algorithms would enable you to meet this challenge.
*   Designed and built multiple big data modules and data pipelines to process large volume. 
*   Genuinely excited about technology and worked on projects from scratch. 

## Must have:

*   7+ years  of hands-on experience in Software Development with a focus on big data and large data pipelines.
*   Minimum 3 years of experience to build services and pipelines using Python.
*   Expertise with a variety of data processing systems, including streaming, event, and batch (Spark, Hadoop/MapReduce)
*   Understanding of at least one NoSQL stores like MongoDB, Elasticsearch, HBase
*   Understanding of how data models, sharding and data location strategies for distributed data stores in large scale high-throughput and high-availability environments and their effect in non-structured text data processing
*   Experience with running scalable & high available systems with AWS or GCP.

## Good to have:

*   Experience with Docker / Kubernetes
*   Exposure with CI/CD
*   Knowledge of Crawling/Scraping

## Additional Information

*   Entire Work From Home
*   Birthday Leave
*   Remote Work
